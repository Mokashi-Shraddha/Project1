{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOo4AyRwxBKUDGa4oxTU9a/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ajpr5RL3_rZN"},"outputs":[],"source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","def preprocess_text(text):\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\S+', '', text)\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    text = text.lower()\n","    words = nltk.word_tokenize(text)\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n","    lemmatizer = WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    text = ' '.join(words)\n","    return text\n"]},{"cell_type":"code","source":[],"metadata":{"id":"zWcUi5vK2_lq"},"execution_count":null,"outputs":[]}]}